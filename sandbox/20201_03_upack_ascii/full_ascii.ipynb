{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedicated-jackson",
   "metadata": {},
   "source": [
    "# Notebook Intentions\n",
    "\n",
    "In this notebook we will attempt to standardize a methodology for converting ascii data and sas programming txt into a list of respondent dictionaries. If successful we can use this method for all data files across all years, and be able to map encoded values to their full form.\n",
    "\n",
    "## Notes\n",
    "\n",
    "Extracting variable names, locations in acsii files and building respondent dictionaries is fairly straightforward and standardized. Converting encoded values to lookup dictionaries is feasibly however the code is somewhat not very pretty or readable. As we move forward we will attempt to clean it up, or abandon it.\n",
    "\n",
    "## Update\n",
    "\n",
    "There are still many exceptions even with a single .dat file. Since all data is stored as text and there is no clear distinction between int, float and string variables it's difficult to make decisions whether leading zeros should be stripped. It doesn't help that some variables are binned floats with string exceptions. We will need to search for existing SAS to python packages to assist as this type of information is likely encoded in the txt file somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(expanduser(\"~\"), \"meps\", \"meps_dev\"))\n",
    "\n",
    "from meps_dev.meps_db.components.populators import BaseComponentsPopulator as bcp\n",
    "from meps_dev.meps_db.components.reference import FYCDF_PUF_LOOKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_text, sas_text = bcp.unpack_data(folder=\"consolidated\", year=2016, year_lookup=FYCDF_PUF_LOOKUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key markers for seperating data\n",
    "std_sas_text = sas_text.split(\"\\n\")\n",
    "input_index = std_sas_text.index(\"* INPUT STATEMENTS;\")\n",
    "format_index = std_sas_text.index(\"* FORMAT STATEMENTS;\")\n",
    "label_index = std_sas_text.index(\"* LABEL STATEMENTS;\")\n",
    "value_index = std_sas_text.index(\"* VALUE STATEMENTS;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build map for extracting ascii text\n",
    "var_name_place = []\n",
    "for row in std_sas_text[input_index:format_index]:\n",
    "    # skip headers\n",
    "    if \"@\" not in row:\n",
    "        continue\n",
    "    split_row = row.strip(\"INPUT\").split()\n",
    "    var_name_place.append(\n",
    "        {\n",
    "            \"name\": split_row[1],\n",
    "            \"start\": int(float(split_row[0].strip(\"@\"))),\n",
    "            \"size\": int(float(split_row[2].strip(\"$\")))\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_descriptions = {}\n",
    "for row in std_sas_text[label_index:value_index]:\n",
    "    if \"=\" not in row:\n",
    "        continue\n",
    "    split_row = row.strip(\"LABEL\").split(\"=\")\n",
    "    var_descriptions[split_row[0].strip()] = split_row[1].replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ascii text\n",
    "data = []\n",
    "row_data = ascii_text.decode(\"utf-8\").split(\"\\r\\n\")\n",
    "for row in row_data[:-1]: # last row is always empty\n",
    "    data_dict = {}\n",
    "    for var_dict in var_name_place:\n",
    "        # SAS starts lists on 1, python on 0\n",
    "        val = row[var_dict[\"start\"]-1:var_dict[\"start\"]-1+var_dict[\"size\"]].strip()\n",
    "        data_dict.update({var_dict[\"name\"]: val})\n",
    "    \n",
    "    data.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build map between input variable names and format variable name\n",
    "input_format_lookup = {}\n",
    "for row in std_sas_text[format_index:label_index-2]:\n",
    "    split_row = row.strip(\"FORMAT\").split()\n",
    "    if 1<=len(split_row)<2 or \"*\" in split_row:\n",
    "        continue\n",
    "    input_format_lookup[split_row[0]] = split_row[1].strip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build lookup of variable name to value statements\n",
    "value_statement_groups = []\n",
    "# group on varaibles\n",
    "value_statement = []\n",
    "for row in std_sas_text[value_index:]:\n",
    "    if \"*\" in row:\n",
    "        continue\n",
    "    # identified header\n",
    "    if \"VALUE\" in row:\n",
    "        # store previous group\n",
    "        value_statement_groups.append(value_statement)\n",
    "        value_statement = [row]\n",
    "    else:\n",
    "        value_statement.append(row)\n",
    "# get last value statement\n",
    "value_statement_groups.append(value_statement)\n",
    "\n",
    "\n",
    "# build proto- map\n",
    "variable_statements_map = {}\n",
    "for group in value_statement_groups:\n",
    "    if len(group)==0:\n",
    "        continue\n",
    "    key = group[0].split()[1]\n",
    "    variable_statements_map[key] = [statement.strip() for statement in group[1:-1]]\n",
    "\n",
    "var_map = {}\n",
    "for var, statement_list in variable_statements_map.items():\n",
    "    # binned values\n",
    "    if any(len(re.findall(r\"\\d - \\d\", statement)) > 0 for statement in statement_list):\n",
    "        vals = []\n",
    "        encode_map = {}\n",
    "        for statement in statement_list:\n",
    "            # classify bin type\n",
    "            if any(marker in statement for marker in {\"YEAR\", \"AGE\", \"WEIGHT\"}):\n",
    "                dtype = \"binned_int\"\n",
    "            if any(marker in statement for marker in {\"$\", \"WAGE\"}):\n",
    "                dtype = \"binned_currency\"\n",
    "            if any(marker in statement for marker in {\"DUID\", \"DUPERSID\"}):\n",
    "                dtype = \"binned_id\"\n",
    "                \n",
    "            if len(re.findall(r\"\\d - \\d\", statement)) == 0:\n",
    "                map_string = re.search(\"'(.*)'\", statement).group(1)\n",
    "                map_string_split = map_string.split()\n",
    "                # handle zero mapping to zero\n",
    "                if map_string_split[0] in {\"0.00\", \"0.000000\"}:\n",
    "                    vals.extend([0])\n",
    "                # handle negative number bins\n",
    "                elif len(re.findall(r\"\\d - -\\d\", statement)) >= 1:\n",
    "                    for substring, sub in [\n",
    "                        (\" - \", \" \"), (\" = \", \" \"), (\"$\", \"\"), (\"'\", \"\"), (\",\", \"\")\n",
    "                    ]:\n",
    "                        statement = statement.replace(substring, sub)\n",
    "                        statement = statement.split(\"=\")[0]\n",
    "                    vals.extend([float(val) for val in statement.split()])\n",
    "                # handle simple exceptions\n",
    "                else:\n",
    "                    encode_map[map_string_split[0]] = \" \".join(map_string_split[1:])\n",
    "            \n",
    "            else:\n",
    "                for substring, sub in [\n",
    "                    (\" - \", \" \"), (\" = \", \" \"), (\"$\", \"\"), (\"'\", \"\"), (\",\", \"\")\n",
    "                ]:\n",
    "                    statement = statement.replace(substring, sub)\n",
    "                    statement = statement.split(\"=\")[0]\n",
    "                vals.extend([float(val) for val in statement.split()])\n",
    "             \n",
    "        var_map[var] = {\n",
    "            \"dtype\": dtype,\n",
    "            \"min\": min(vals),\n",
    "            \"max\": max(vals),\n",
    "            \"encode_map\": encode_map\n",
    "        }\n",
    "    # hot enocoded\n",
    "    else:\n",
    "        encode_map = {}\n",
    "        for statement in statement_list:\n",
    "            statement_list = statement.split(\"=\")\n",
    "            key = statement_list[0].strip().replace(\"'\", \"\")\n",
    "            val = \" \".join(statement_list[1:]).replace(key, \"\", 1).replace(\"'\", \"\").strip()\n",
    "            #map_string_split = statement_list[1].split()\n",
    "            #map_string_split = [split.replace(\"'\", \"\") for split in map_string_split]\n",
    "            #encode_map[map_string_split[0]] = \" \".join(map_string_split[1:])\n",
    "            encode_map[key]= val\n",
    "        var_map[var] = {\n",
    "            \"dtype\": \"categorical\",\n",
    "            \"encode_map\": encode_map\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-circular",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_resp = {}\n",
    "for var, value in data[0].items():\n",
    "    format_var_name = input_format_lookup[var]\n",
    "    var_params = var_map[format_var_name]\n",
    "    if var_params[\"dtype\"] == \"categorical\":\n",
    "        try:\n",
    "            cleaned_resp[var] = var_params[\"encode_map\"][value]\n",
    "        except KeyError:\n",
    "            \n",
    "            cleaned_resp[var] = var_params[\"encode_map\"][value.strip(\"0\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-metadata",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
